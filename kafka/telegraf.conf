[agent]
  interval = "5s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  hostname = "localhost"
  omit_hostname = false

# Input plugin for MQTT
[[inputs.mqtt_consumer]]
  servers = ["192.168.0.160:1883"]
  topics = [
    "data/#",
    "status/#",
    "alarm/#"
  ]
  qos = 0
  connection_timeout = "30s"
  persistent_session = false
  client_id = ""
  data_format = "json"
  json_string_fields = ["status","wos","d_str1","d_str2"]

# Output plugin for Kafka
[[outputs.kafka]]
  brokers = ["kafka1:9092", "kafka2:9093", "kafka3:9094"]
  topic_tag = "kafka_topic"

# Processors to route MQTT topics to specific Kafka topics
[[processors.starlark]]
  namepass = ["mqtt_consumer"]
  source = '''
def apply(metric):
  if "data" in metric.tags.get("topic", ""):
    metric.tags["kafka_topic"] = "topic_data"
  elif "status" in metric.tags.get("topic", ""):
    metric.tags["kafka_topic"] = "topic_status"
  elif "alarm" in metric.tags.get("topic", ""):
    metric.tags["kafka_topic"] = "topic_alarm"
  return metric
'''

# # Processors to add keys based on MQTT topics for a single Kafka topic
# [[processors.starlark]]
#   namepass = ["mqtt_consumer"]
#   source = '''
# def apply(metric):
#   topic = metric.tags.get("topic", "")
#   if "data" in topic:
#     metric.tags["kafka_key"] = "key_data"
#   elif "status" in topic:
#     metric.tags["kafka_key"] = "key_status"
#   elif "alarm" in topic:
#     metric.tags["kafka_key"] = "key_alarm"
#   return metric
# '''
# # Output plugin for Kafka
# [[outputs.kafka]]
#   brokers = ["kafka1:9092", "kafka2:9093", "kafka3:9094"]
#   topic = "single_topic"  # Use a single topic for all messages
#   routing_tag = "kafka_key"